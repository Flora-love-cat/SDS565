{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given training vectors $x_i \\in \\mathbb{R}^p, i=1,..., n$ in two classes, and a\n",
    "vector $y \\in \\{1, -1\\}^n$, \n",
    "\n",
    "our goal is to find $w \\in \\mathbb{R}^p$ and $b \\in \\mathbb{R}$ such that the prediction given by $\\text{sign} (w^T\\phi(x) + b)$ is correct for most samples.\n",
    "\n",
    "SVC solves the following **primal problem**:\n",
    "\n",
    "$$\n",
    "\\min_ {w, b, \\zeta} \\frac{1}{2} w^T w + C \\sum_{i=1}^{n} \\zeta_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{subject to }  y_i (w^T \\phi (x_i) + b) \\geq 1 - \\zeta_i,\\\\  \\zeta_i \\geq 0, i=1, ..., n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, we're trying to maximize the margin (by minimizing $||w||^2 = w^Tw$), \n",
    "\n",
    "while incurring a penalty when a sample is misclassified or within the margin boundary. \n",
    "\n",
    "Ideally, the value $y_i(w^T \\phi (x_i) + b)$ would be $\\geq 1$ for all samples, which indicates a perfect prediction. \n",
    "\n",
    "But problems are usually not always perfectly separable with a hyperplane, \n",
    "\n",
    "so we allow some samples to be at a distance $\\zeta_i$ from their correct margin boundary. \n",
    "\n",
    "The penalty term $C$ controls the strength of this penalty, \n",
    "\n",
    "and as a result, acts as an inverse regularization parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **dual problem** to the primal is\n",
    "\n",
    "\n",
    "$$\n",
    "\\min_{\\alpha} \\frac{1}{2} \\alpha^T Q \\alpha - e^T \\alpha\n",
    "$$\n",
    "$$\n",
    "   \\textrm {subject to }  y^T \\alpha = 0\\\\\n",
    "    0 \\leq \\alpha_i \\leq C, i=1, ..., n\n",
    "$$\n",
    "\n",
    "where $e$ is the vector of all ones,\n",
    "\n",
    "$Q$ is an $n$ by $n$ positive semidefinite matrix,\n",
    "\n",
    "$Q_{ij} \\equiv y_i y_j K(x_i, x_j)$, where $K(x_i, x_j) = \\phi (x_i)^T \\phi (x_j)$ is the kernel. \n",
    "\n",
    "The terms $\\alpha_i$ are dual coefficients, upper-bounded by $C$.\n",
    "\n",
    "This dual representation highlights the fact that training vectors are implicitly mapped into a higher (maybe infinite) dimensional space by the function $\\phi$\n",
    "\n",
    "see [kernel trick](https://en.wikipedia.org/wiki/Kernel_method).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the optimization problem is solved, \n",
    "\n",
    "predicted class is sign of output of decision function:\n",
    "\n",
    "$$\\text{sign}\\left[\\sum_{i\\in SV} y_i \\alpha_i K(x_i, x) + b\\right]$$\n",
    "\n",
    "\n",
    "We only need to sum over the support vectors (i.e. the samples that lie within the margin) \n",
    "\n",
    "because the dual coefficients $\\alpha_i$ are zero for the other samples.\n",
    "\n",
    "These parameters can be accessed through the attributes ``dual_coef_``\n",
    "which holds the product $y_i \\alpha_i$, \n",
    "\n",
    "``support_vectors_`` which holds the support vectors, \n",
    "\n",
    "and ``intercept_`` which holds the independent term $b$\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
