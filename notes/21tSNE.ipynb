{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE (Student-t Stochastic Neighborhood Embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE is just a visualization method, give intuition for high dimensional embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### idea and interpreation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "idea: minimize divergence between 2 distributions\n",
    "\n",
    "- a distribution that measures pairwise similarities of **corresponding visualization vectors** (**word embedding vectors**)\n",
    "\n",
    "\n",
    "interpretation: close will be close, far will be far\n",
    "\n",
    "- if word embedding vector $\\phi(w_i) \\in \\mathbb{R}^d$ is very **close** to $\\phi(w_j)$,\n",
    "\n",
    "    then visualization vector $y_i \\in \\mathbb{R}^2$ will also be **close** to $y_j$\n",
    "\n",
    "- while word embedding vector $\\phi(w_i)$ is very **far** to $\\phi(w_j)$,\n",
    "\n",
    "    then visualization vector $y_i$ will also be **far** to $y_j$\n",
    "\n",
    "\n",
    "- long distances maybe stretched further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. form a Gaussian kernel over vocabulary based on embedding vectors $\\phi(w_i) \\in \\mathbb{R}^d$\n",
    "\n",
    "\n",
    "2. scale and symmetrize, produce a matrix $P=[P_{ij}]$\n",
    "\n",
    "\n",
    "3. represent word $i$ by visualization vectors $y_i \\in \\mathbb{R}^2$ (suppose we want to visualize in 2D)\n",
    "\n",
    "    use a heavy-tailed Student t-distribution with df=1\n",
    "    \n",
    "\n",
    "4. solve optimal $y_i$ using stochastic gradient descent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **first**, for each word $w_i$, compute a language model\n",
    "\n",
    "    a conditional distribution model: Given word $i$ the probability of word $j$ is word $i$'s neighbor:\n",
    "\n",
    "$$\n",
    "P_{j|i} \\propto \\exp\\left [ -\\frac{\\left \\| \\phi (w_i)-\\phi (w_j)\\right \\|^2}{2 h_i^2} \\right ]\n",
    "$$\n",
    "\n",
    "\n",
    "derived from Gaussian Kernel:\n",
    "\n",
    "$$\n",
    "P_{j|i} =\\frac{\\exp\\left [ -\\frac{\\left \\| \\phi (w_i)-\\phi (w_j)\\right \\|^2}{2 h_i^2} \\right ]}{\\sum _k\\exp\\left [ -\\frac{\\left \\| \\phi (w_i)-\\phi (w_k)\\right \\|^2}{2 h_i^2} \\right ]} \n",
    "$$\n",
    "\n",
    "where $k$ is the number of words in the corpus\n",
    "\n",
    "$h_i$ is for scaling, bandwidth for word $i$, which set perplexity to be 10\n",
    "\n",
    "let all probabilities on the same scale"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "perplexity is often set between 5-50:\n",
    "\n",
    "$$\n",
    "e^{H(j|i)} \\approx 10\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "where $H(j|i)$ is entropy\n",
    "\n",
    "$$\n",
    "H(j|i)=\\sum_j P_{j|i} \\log(P_{j|i})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the language model $P_{j|i}$ produces non-linearity\n",
    "\n",
    "a simple way of symmetrizing:\n",
    "\n",
    "$$\n",
    "P_{ij} = \\frac{1}{2} (P_{j|i}+P_{i|j})\n",
    "$$\n",
    "\n",
    "where $P_{ij}$ is entry of a symmetric matrix $P$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for Kernel PCA:\n",
    "\n",
    "- first embed data on high dimensional space, get $P_{j|i}$\n",
    "\n",
    "\n",
    "- then do SVD on $P_{j|i}$ to project onto finite space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **second**, form Student-t distribution based on visualization vectors $y_i \\in \\mathbb{R}^2$ \n",
    "\n",
    "has fatter tails than Gaussian, encourage close vectors to be more close, spread vectors to be more spread\n",
    "\n",
    "each entry of matrix $Q$ is proportional to :\n",
    "\n",
    "$$\n",
    "Q_{ij} \\propto \\left ( 1+\\left \\| y_i -y_j \\right \\|_2^2 \\right )^{-1}\n",
    "$$\n",
    "\n",
    "derived from\n",
    "\n",
    "$$\n",
    "Q_{ij} = \\frac{\\left ( 1+\\left \\| y_i -y_j \\right \\|_2^2 \\right )^{-1}}{\\sum _{k \\neq l}\\left ( 1+\\left \\| y_k -y_l \\right \\|_2^2 \\right )^{-1}}\n",
    "$$\n",
    "\n",
    "where denominator is sum over rows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **finally**, run stochastic gradient descent to optimize Kullback-Leibler divergence between matrix $P$ and $Q$  over vectors $y_i$ \n",
    "\n",
    "    Kullback-Leibler divergence is a kind of non-linear loss\n",
    "\n",
    "$$\n",
    "\\hat y = \\arg \\min_{y} \\sum_{ij} P_{ij} \\log \\left ( \\frac{P_{ij}}{Q_{ij}} \\right ) \n",
    "$$\n",
    "\n",
    "where $P_{ij}$ is observed, $Q_{ij}$ is unknown (we are learning), dictated by $\\hat y_i$\n",
    "\n",
    "$$\n",
    "P_{ij} = \\frac{1}{2} (P_{j|i}+P_{i|j})\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student t-distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- general form of Student t-distribution\n",
    "\n",
    "$$\n",
    "f(t)=\\frac{\\Gamma (\\frac{v+1}{2})}{\\sqrt{v\\pi}\\Gamma (\\frac{v}{2})}\\left ( 1+\\frac{t^2}{v} \\right )^{-\\frac{v+1}{2}}\n",
    "$$\n",
    "\n",
    "where $v$ is degree of freedom\n",
    "\n",
    "\n",
    "- simplified form with 1 degree of freedom, also called **Cauchy distribution** \n",
    "\n",
    "$$\n",
    "f(t)=\\frac{1}{\\pi}\\left ( 1+t^2 \\right )^{-1}\n",
    "$$\n",
    "\n",
    "thus the language model $P_{j|i}$ is $e^{-t^2}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15a162aef56c374f20e075fbc4add75b55bd097c903a763d3b0c4cccda1c0caf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
