{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generation vs. discrimination"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Generative models and discriminative models are two different types of models in machine learning\n",
    "\n",
    "|                    | Generative Models       | Discriminative Models          |\n",
    "|--------------------|-------------------------|--------------------------------|\n",
    "| modelling goal               | joint probability $p(x, y)$           | posterior probability $p(y\\|x)$          |\n",
    "| Approach           | Bayes' Rule to calculate $p(y|x)$        | Minimize classification errors |\n",
    "| Examples           | NaÃ¯ve Bayes, Hidden Markov Models (HMM), Probabilistic Context-Free Grammar (PCFG)  | Logistic Regression, SVM, Decision Tree   |\n",
    "| Pros               | deal with unlabeled data  | Optimize classification accuracy |\n",
    "| Cons               | More challenging task   | Sensitive to feature selection, Overfitting when sparse data |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes rule"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "posterior probability of $y$ given $\\mathbf{x}$ is the joint probability of $\\mathbf{x}$  and $y$ divided by marginal probability of $\\mathbf{x}$ \n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "&\\mathbb{P}(y|\\mathbf{x})=\\frac{\\mathbb{P}(\\mathbf{x}|y)\\mathbb{P}(y)}{\\mathbb{P}(\\mathbf{x})}\n",
    "\\\\[1em]\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbb{P}(y)=\\left\\{\\begin{matrix}\n",
    "\\pi_0 & y=0\\\\ \\pi_1 & y=1\\end{matrix}\\right.\\ $,  $\\mathbb{P}(\\mathbf{x})=\\sum_{j}\\mathbb{P}(\\mathbf{x}|y=j)\\mathbb{P}(y=j)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling choice"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Naive Bayes:\n",
    "\n",
    "    conditional probability of $\\mathbf{x}\\in \\mathbb{R}^d$ given $y \\in \\mathbb{R}$ is the cumulative product of conditional probability of $d$ independent coordinate $\\mathbf{x}_j$ given $y$\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\mathbf{x}|y)=\\prod_{j=1}^{d}\\mathbb{P}(\\mathbf{x}_j|y)\n",
    "$$\n",
    "\n",
    "\n",
    "- we can pick any univariate distribution for $\\mathbf{x}_j|y$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gaussian:  explore in HW3 Q2\n",
    "\n",
    "    distribution of $\\mathbf{x}\\in \\mathbb{R}^d$ conditioned on $y\\in \\mathbb{R}$ is normal with mean $\\mathbf{\\mu}_y \\in \\mathbb{R}^d$ and covariance $\\Sigma_y \\in \\mathbb{R}^{d \\times d}$\n",
    "\n",
    "$$\n",
    "\\mathbf{x}|y \\sim N(\\mathbf{\\mu}_y,\\Sigma_y)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# decision rule of Naive Bayes classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decision rule of binary classification is:\n",
    "\n",
    "$$\n",
    "\\mathbb{1}[\\mathbb{P}(y=1|\\mathbf{x})>\\mathbb{P}(y=0|\\mathbf{x})]\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use Bayes rule to simplify"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "&\\frac{\\mathbb{P}(\\mathbf{x}|y=1)\\mathbb{P}(y=1)}{\\mathbb{P}(\\mathbf{x})} > \\frac{\\mathbb{P}(\\mathbf{x}|y=0)\\mathbb{P}(y=0)}{\\mathbb{P}(\\mathbf{x})}\\\\[1em]\n",
    "\n",
    "& \\mathbb{P}(\\mathbf{x}|y=1)\\mathbb{P}(y=1)>\\mathbb{P}(\\mathbf{x}|y=0)\\mathbb{P}(y=0)\\\\[1em]\n",
    "\n",
    "& \\ln \\left[\\mathbb{P}(\\mathbf{x}|y=1)\\mathbb{P}(y=1)\\right] > \\ln \\left[\\mathbb{P}(\\mathbf{x}|y=0)\\mathbb{P}(y=0) \\right] \\\\[1em]\n",
    "\n",
    "&\\ln \\left[\\mathbb{P}(\\mathbf{x}|y=1) \\right] + \\ln \\left[ \\mathbb{P}(y=1)\\right] > \\ln \\left[\\mathbb{P}(\\mathbf{x}|y=0) \\right] + \\ln \\left[ \\mathbb{P}(y=0)\\right]\\\\[1em]\n",
    "\n",
    "&\\ln(\\mathbb{P}(\\mathbf{x}|y=1))-\\ln(\\mathbb{P}(\\mathbf{x}|y=0))>\\ln(\\mathbb{P}(y=0))-\\ln(\\mathbb{P}(y=1))\\\\[1em]\n",
    "\n",
    "&\\ln\\left[\\frac{\\mathbb{P}(\\mathbf{x}|y=1)}{\\mathbb{P}(\\mathbf{x}|y=0)}\\right]>\\ln\\left[\\frac{\\mathbb{P}(y=0)}{\\mathbb{P}(y=1)}\\right]\\\\[1em]\n",
    "\n",
    "&\\ln \\left [ \\frac{\\prod_{j=1}^{d}\\mathbb{P}(\\mathbf{x}_j|y=1)}{\\prod_{j=1}^{d}\\mathbb{P}(\\mathbf{x}_j|y=0)} \\right ] > \\ln \\left ( \\frac {\\pi_0}{\\pi_1}\\right )\\\\[1em]\n",
    "\n",
    "&\\sum_{j=1}^{d}\\ln\\left[\\frac{\\mathbb{P}(\\mathbf{x}_j|y=1)}{\\mathbb{P}(\\mathbf{x}_j|y=0)} \\right]> \\ln \\left ( \\frac {\\pi_0}{\\pi_1}\\right )\\\\[1em]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "label $y\\in \\left \\{0,1 \\right \\}$, a letter is a Spam or NOT a Spam\n",
    "\n",
    "features $\\mathbf{x}\\in \\mathbb{R}^d$, each coordinate is independent and conditioned on $y$\n",
    "\n",
    "each feature $\\mathbf{x}_j\\sim Bernoulli(p_j)$ is occurrence of a word $j$ in an email, independent with each other\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_j=\\left\\{\\begin{matrix}\n",
    "1 & \\text{word j appears} \\\\ \n",
    "0 &\\ \\text{word j not appear}\n",
    "\\end{matrix}\\right.\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plug in\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\mathbf{x}_j|y)=(p_{j,y})^{\\mathbf{x}_j}\\ (1-p_{j,y})^{1-\\mathbf{x}_j}\n",
    "$$\n",
    "\n",
    "to the previous decision rule\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^{d}\\ln\\left[\\frac{\\mathbb{P}(\\mathbf{x}_j|y=1)}{\\mathbb{P}(\\mathbf{x}_j|y=0)} \\right]> \\ln \\left ( \\frac {\\pi_0}{\\pi_1}\\right )\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^{d}\\ln\\left[\\frac{(p_{j,1})^{\\mathbf{x}_j}\\ (1-p_{j,1})^{1-\\mathbf{x}_j}}{(p_{j,0})^{\\mathbf{x}_j}\\ (1-p_{j,0})^{1-\\mathbf{x}_j}} \\right]> \\ln \\left ( \\frac {\\pi_0}{\\pi_1}\\right )\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simplify this,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\n",
    "&\\sum_{j=1}^{d}\\ln\\left[ \\left ( \\frac{p_{j,1}}{p_{j,0}}\\right )^{\\mathbf{x}_j}\\left ( \\frac{1-p_{j,1}}{1-p_{j,0}}\\right )^{1-\\mathbf{x}_j}\\right]> \\ln \\left ( \\frac {\\pi_0}{\\pi_1}\\right )\\\\[1em]\n",
    "\n",
    "&\\sum_{j=1}^{d}\\ln\\left[ \\left ( \\frac{p_{j,1}}{p_{j,0}}\\right )^{\\mathbf{x}_j}\\left ( \\frac{1-p_{j,0}}{1-p_{j,1}}\\right )^{\\mathbf{x}_j}\\left ( \\frac{1-p_{j,1}}{1-p_{j,0}} \\right )\\right]> \\ln \\left ( \\frac {\\pi_0}{\\pi_1}\\right )\\\\[1em]\n",
    "\n",
    "&\\sum_{j=1}^{d}\\ln\\left[ \\left ( \\frac{p_{j,1}}{p_{j,0}}\\frac{1-p_{j,0}}{1-p_{j,1}}\\right )^{\\mathbf{x}_j}\\left ( \\frac{1-p_{j,1}}{1-p_{j,0}} \\right )\\right]> \\ln \\left ( \\frac {\\pi_0}{\\pi_1}\\right )\\\\[1em]\n",
    "\n",
    "&\\sum_{j=1}^{d}\\ln\\left[ \\left ( \\frac{p_{j,1}}{1-p_{j,1}}\\frac{1-p_{j,0}}{p_{j,0}}\\right )^{\\mathbf{x}_j}\\left ( \\frac{1-p_{j,1}}{1-p_{j,0}} \\right )\\right]> \\ln \\left ( \\frac {\\pi_0}{\\pi_1}\\right )\\\\[1em]\n",
    "\n",
    "&\\sum_{j=1}^{d}\\ln\\left ( \\frac{p_{j,1}}{1-p_{j,1}}\\frac{1-p_{j,0}}{p_{j,0}}\\right )^{\\mathbf{x}_j}+\\sum_{j=1}^{d}\\ln\\left( \\frac{1-p_{j,1}}{1-p_{j,0}} \\right) > \\ln \\left ( \\frac {\\pi_0}{\\pi_1}\\right )\\\\[1em]\n",
    "\n",
    "\n",
    "&\\sum_{j=1}^{d}\\mathbf{x}_j\\ln\\left ( \\frac{p_{j,1}}{1-p_{j,1}}\\frac{1-p_{j,0}}{p_{j,0}}\\right )+\\sum_{j=1}^{d}\\ln\\left( \\frac{1-p_{j,1}}{1-p_{j,0}} \\right) > \\ln \\left ( \\frac {\\pi_0}{\\pi_1}\\right )\\\\[1em]\n",
    "\n",
    "\n",
    "&\\sum_{j=1}^{d}\\mathbf{x}_j\\ln\\left ( \\frac{p_{j,1}}{1-p_{j,1}}\\frac{1-p_{j,0}}{p_{j,0}}\\right )> \\ln \\left ( \\frac {\\pi_0}{\\pi_1}\\right )-\\sum_{j=1}^{d}\\ln\\left( \\frac{1-p_{j,1}}{1-p_{j,0}} \\right) \\\\[1em]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decision rule is a **linear classifier**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set $\\mathbf{\\beta} \\in \\mathbb{R}^d$, $\\mathbf{\\beta}_j=\\ln\\left ( \\frac{p_{j,1}}{1-p_{j,1}}\\frac{1-p_{j,0}}{p_{j,0}}\\right )\\in \\mathbb{R}$, $C=\\ln \\left ( \\frac {\\pi_0}{\\pi_1}\\right )-\\sum_{j=1}^{d}\\ln\\left( \\frac{1-p_{j,1}}{1-p_{j,0}} \\right)\\in \\mathbb{R}$\n",
    "\n",
    "then the decision rule can be written as\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^{d}\\mathbf{x}_j \\mathbf{\\beta}_j > C\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a word appears in Spam or not depends on sign of $\\mathbf{\\beta}_j$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "odds ratio for a word appear in a Spam is $\\frac{p_{j,1}}{1-p_{j,1}}$\n",
    "\n",
    "odds ratio for a word appear in a NOT-Spam email is $\\frac{p_{j,0}}{1-p_{j,0}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in practice"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in practice, can't directly use decision rule bc don't know distribution of $x_j$, i.e., $p_{j,y}=\\mathbb{P}(\\mathbf{x}_j|y)$\n",
    "\n",
    "- but can can estimate $p_{j,y}$ from training data, then use **plug-in** $\\ln\\left( \\frac{1-p_{j,1}}{1-p_{j,0}} \\right)$ back to linear classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- words may dependent on each other, so $\\mathbf{x_j}$ is not independent\n",
    "\n",
    "    e.g., in the Email of Disney, word \"Micky\" and word \"Mouse\" may always appear at the same time"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "263.73px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "15a162aef56c374f20e075fbc4add75b55bd097c903a763d3b0c4cccda1c0caf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
